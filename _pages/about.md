---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a junior student at Turing class, Peking university, where I am fortunate to be advised by [Prof. Liwei Wang](http://www.liweiwang-pku.com/). I work closely with and learn a lot from [Di He](https://www.microsoft.com/en-us/research/people/dihe/) at MSRA. This summer, I worked as an intern in [Cho-Jui Hsieh](http://web.cs.ucla.edu/~chohsieh/)'s group at UCLA remotely.

My research area is machine learning, with special interests in models and algorithms inspired by theoretical insights. Recently, my work focuses on the Transformer and its variants [1]. I'm also interested in Transformers in vision and graph tasks.

Publication
=====
[1] **Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding** (NeurIPS 2021)  
   Shengjie Luo\*, **Shanda Li**\*, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu  
   <font face="Times New Roman">We propose a new method to accelerate attention calculation for Transformers with relative positional encoding (RPE) on top of the kernelized attention by utilizing Fast Fourier Transform (FFT). Our method also helps to stablize training of kernelized attention models.</font>
   
Education
=====

**Peking University**, Beijing, China

* B.S. in Computer Science and Technology. 2018 - 2022 (expected).

Service and leadership
=====  

2020.9 - 2021.9, **President**, EECS Studentsâ€™ Association for Science and Technology, Peking University.
