---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a first-year Ph.D. student in Machine Learning Department, School of Computer Science, Carnegie Mellon University, advised by [Prof. Yiming Yang](https://www.cs.cmu.edu/~./yiming/). I received my Bachelor's degree in computer science from Peking university, where I was fortunate to work with [Prof. Di He](https://dihe-pku.github.io/) and [Prof. Liwei Wang](http://www.liweiwang-pku.com/). I also worked as an intern in [Prof. Cho-Jui Hsieh](http://web.cs.ucla.edu/~chohsieh/)'s group at UCLA remotely in 2021.

My research area is machine learning. Recently, my work focuses on machine learning for science. I'm also interested in attention-based models and the Transformer architecture.

My detailed CV can be found [here](https://lithiumda.github.io/files/CV.pdf).

Selected Publications
=====
[1] **Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding** (NeurIPS 2021) [[PDF](https://arxiv.org/abs/2106.12566)]  
  Shengjie Luo\*, **Shanda Li**\*, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu  

[2] **Your Transformer May Not be as Powerful as You Expect** (NeurIPS 2022) [[PDF](https://arxiv.org/abs/2205.13401)]  
  Shengjie Luo\*, **Shanda Li**\*, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, Di He

[3] **Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?** (NeurIPS 2022) [[PDF](https://arxiv.org/abs/2206.02016)]  
  Chuwei Wang\*, **Shanda Li**\*, Di He, Liwei Wang  

<!-- [4] **Can Vision Transformers Perform Convolution?** (Preprint) [[PDF](https://arxiv.org/abs/2111.01353)]  
  **Shanda Li**, Xiangning Chen, Di He, Cho-Jui Hsieh   -->
   
<!-- [5] **Learning Physics-Informed Neural Networks without Stacked Back-propagation** (Preprint)  [[PDF](https://arxiv.org/abs/2202.09340)]  
  Di He\*, Wenlei Shi\*, **Shanda Li**\*, Xiaotian Gao, Jia Zhang, Jiang Bian, Liwei Wang, Tie-Yan Liu -->